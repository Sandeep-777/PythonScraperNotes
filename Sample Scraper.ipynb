{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import the libraries.</b>\n",
    "<br>\n",
    "<b>BeautifulSoup</b> this is used to retrive necessary information from the html pages.\n",
    "<br>\n",
    "<b>Request, urlopen</b> this is used to retrive necessary information from the html pages. urlopen opens the url.\n",
    "<br>\n",
    "<b>Pandas</b> this is used to store the data in csv format, clean data and manage data.\n",
    "<br>\n",
    "<b>re</b> this is used for regular expressions to manipulate strings, find keywords and so on.\n",
    "<br>\n",
    "<b>pickle</b> this is used to store python objects in files.\n",
    "<br>\n",
    "<b>os</b> this is used to do operation system dependent functions. Here we will be using it to create folder to store files.\n",
    "<br>\n",
    "<b>glob</b> this is used to find pathnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the urls to scrape. Usually most webpages list products or information page by page. Just increment the page number and store each url in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = 'http://example.webscraping.com/places/default/index/'\n",
    "urls = []\n",
    "for i in range(1, 5):\n",
    "    urls.append(baseUrl+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the html data from urls and display error if something goes wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConnection(my_url):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = Request(my_url, headers=hdr)\n",
    "    try:\n",
    "        uClient = urlopen(req)\n",
    "        rawData = uClient.read()\n",
    "        # close the content\n",
    "        uClient.close()\n",
    "    except HTTPError:\n",
    "        print(\"Http error\")\n",
    "        rawData = False\n",
    "    return rawData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store all the unprocessed html data into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.webscraping.com/places/default/index/1\n",
      "http://example.webscraping.com/places/default/index/2\n",
      "http://example.webscraping.com/places/default/index/3\n",
      "http://example.webscraping.com/places/default/index/4\n"
     ]
    }
   ],
   "source": [
    "rawList = []\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    rawList.append(getConnection(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the links with relevant information from the page. Check if link are relative or absolute, if relative add the base url in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linklist = []\n",
    "base_link = 'http://example.webscraping.com'\n",
    "for rawdata in rawList:\n",
    "    bs = BeautifulSoup(rawdata, 'html.parser')\n",
    "    container = bs.find('div', {'id': 'results'})\n",
    "    link_items = container.select('td')\n",
    "    for link_item in link_items:\n",
    "        if link_item.find('a') is not None:\n",
    "            link = link_item.find('a')['href']\n",
    "            if link.startswith('/'):\n",
    "                link = base_link + link\n",
    "            linklist.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect html data from the links extracted and store them in a variable. In this sample we are only storing first five variables. You can choose to store file, or just store it in variable according to the need. If there are lots of urls and you might need to scrape other information in future, you can store it in files. You can store whole array or individual files like shown here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollection = []\n",
    "for url in linklist[:5]:\n",
    "    foldername = 'rawdata'\n",
    "    filename = url.split('/')[-1]+'.txt'\n",
    "    if not os.path.isdir(foldername):\n",
    "        os.mkdir(foldername)\n",
    "    data = getConnection(url)\n",
    "    with open(foldername+'/'+filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    datacollection.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after you have stored the files, you can run this code to get the data back in variables. This is useful when you have to turn off pc and resume scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(foldername+'/*.txt')\n",
    "datacollection = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'rb') as f:\n",
    "        datacollection.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now extract the information you need from the stored files and store them as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = []\n",
    "for data in datacollection:\n",
    "    country_item = {}\n",
    "    bs = BeautifulSoup(data, 'html.parser')\n",
    "    table = bs.select_one('table')\n",
    "    rows = table.select('tr')\n",
    "    for row in rows:\n",
    "        tds = row.select('td')\n",
    "        row_title = tds[0].get_text().strip()\n",
    "        if row_title.endswith(':'):\n",
    "            row_title = row_title[:-1]\n",
    "        row_value = tds[1].get_text().strip()\n",
    "        country_data = {row_title:row_value}\n",
    "        country_item.update(country_data)\n",
    "    country_list.append(country_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now store the data in csv format using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                           Area       Capital Continent    Country  \\\n",
      "0  2,766,890 square kilometres  Buenos Aires        SA  Argentina   \n",
      "1     29,800 square kilometres       Yerevan        AS    Armenia   \n",
      "2        193 square kilometres    Oranjestad        NA      Aruba   \n",
      "3  7,686,850 square kilometres      Canberra        OC  Australia   \n",
      "4     83,858 square kilometres        Vienna        EU    Austria   \n",
      "\n",
      "  Currency Code Currency Name Iso             Languages National Flag  \\\n",
      "0           ARS          Peso  AR  es-AR,en,it,de,fr,gn                 \n",
      "1           AMD          Dram  AM                    hy                 \n",
      "2           AWG       Guilder  AW           nl-AW,es,en                 \n",
      "3           AUD        Dollar  AU                 en-AU                 \n",
      "4           EUR          Euro  AT        de-AT,hr,hu,sl                 \n",
      "\n",
      "                Neighbours Phone  Population Postal Code Format  \\\n",
      "0           CL BO UY PY BR    54  41,343,201           @####@@@   \n",
      "1              GE IR AZ TR   374   2,968,000             ######   \n",
      "2                            297      71,566                      \n",
      "3                             61  21,515,754               ####   \n",
      "4  CH DE HU SK CZ IT SI LI    43   8,205,000               ####   \n",
      "\n",
      "        Postal Code Regex  Tld  \n",
      "0  ^([A-Z]\\d{4}[A-Z]{3})$  .ar  \n",
      "1               ^(\\d{6})$  .am  \n",
      "2                          .aw  \n",
      "3               ^(\\d{4})$  .au  \n",
      "4               ^(\\d{4})$  .at  >\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(country_list)\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use additional function to filter the data as required like to separate first and last name, or to camel case or to convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sqkm_acre(area):\n",
    "    area = area.replace(',', '')\n",
    "    sqkm = re.search(r'[0-9]+', area).group()\n",
    "    return float(sqkm)*247.10538147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply filter and store the dataframe as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Area in acre'] = df['Area'].apply(lambda x: convert_sqkm_acre(x))\n",
    "df.to_csv('countries.csv', index=False, encoding='UTF-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
